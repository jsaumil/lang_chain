{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c617d9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c95a8a85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "model_id = \"cardiffnlp/twitter-roberta-base-sentiment\"\n",
    "save_dir = r\"F:\\python\\model\\cardiffnlp_sentiment\"  # your custom folder\n",
    "\n",
    "# Download and save locally\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.save_pretrained(save_dir)\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_id)\n",
    "model.save_pretrained(save_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97846e80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1) positive 0.8466\n",
      "2) neutral 0.1458\n",
      "3) negative 0.0076\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "import csv\n",
    "import urllib.request\n",
    "\n",
    "# Preprocess text (username and link placeholders)\n",
    "def preprocess(text):\n",
    "    new_text = []\n",
    " \n",
    " \n",
    "    for t in text.split(\" \"):\n",
    "        t = '@user' if t.startswith('@') and len(t) > 1 else t\n",
    "        t = 'http' if t.startswith('http') else t\n",
    "        new_text.append(t)\n",
    "    return \" \".join(new_text)\n",
    "\n",
    "# Tasks:\n",
    "# emoji, emotion, hate, irony, offensive, sentiment\n",
    "# stance/abortion, stance/atheism, stance/climate, stance/feminist, stance/hillary\n",
    "\n",
    "task='sentiment'\n",
    "MODEL = f\"cardiffnlp/twitter-roberta-base-{task}\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "\n",
    "# download label mapping\n",
    "labels=[]\n",
    "mapping_link = f\"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/{task}/mapping.txt\"\n",
    "with urllib.request.urlopen(mapping_link) as f:\n",
    "    html = f.read().decode('utf-8').split(\"\\n\")\n",
    "    csvreader = csv.reader(html, delimiter='\\t')\n",
    "labels = [row[1] for row in csvreader if len(row) > 1]\n",
    "\n",
    "# PT\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
    "model.save_pretrained(MODEL)\n",
    "\n",
    "text = \"Good night 😊\"\n",
    "text = preprocess(text)\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "output = model(**encoded_input)\n",
    "scores = output[0][0].detach().numpy()\n",
    "scores = softmax(scores)\n",
    "\n",
    "# # TF\n",
    "# model = TFAutoModelForSequenceClassification.from_pretrained(MODEL)\n",
    "# model.save_pretrained(MODEL)\n",
    "\n",
    "# text = \"Good night 😊\"\n",
    "# encoded_input = tokenizer(text, return_tensors='tf')\n",
    "# output = model(encoded_input)\n",
    "# scores = output[0][0].numpy()\n",
    "# scores = softmax(scores)\n",
    "\n",
    "ranking = np.argsort(scores)\n",
    "ranking = ranking[::-1]\n",
    "for i in range(scores.shape[0]):\n",
    "    l = labels[ranking[i]]\n",
    "    s = scores[ranking[i]]\n",
    "    print(f\"{i+1}) {l} {np.round(float(s), 4)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cdbf40b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: I love working with Hugging Face!\n",
      "Sentiment: Positive, Score: 0.9908\n",
      "\n",
      "Text: This is the worst experience ever.\n",
      "Sentiment: Negative, Score: 0.9766\n",
      "\n",
      "Text: It's okay, not great but not bad either.\n",
      "Sentiment: Positive, Score: 0.7400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
    "\n",
    "# Path to your local model folder\n",
    "model_path = r\"F:\\python\\model\\cardiffnlp_sentiment\"\n",
    "\n",
    "# Load tokenizer and model from local directory\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "\n",
    "# Create sentiment analysis pipeline\n",
    "sentiment_pipeline = pipeline(\"text-classification\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Test sentences\n",
    "sentences = [\n",
    "    \"I love working with Hugging Face!\",\n",
    "    \"This is the worst experience ever.\",\n",
    "    \"It's okay, not great but not bad either.\"\n",
    "]\n",
    "\n",
    "# Map Hugging Face labels to real sentiments\n",
    "label_map = {\n",
    "    \"LABEL_0\": \"Negative\",\n",
    "    \"LABEL_1\": \"Neutral\",\n",
    "    \"LABEL_2\": \"Positive\"\n",
    "}\n",
    "\n",
    "# Run and print results\n",
    "for text in sentences:\n",
    "    result = sentiment_pipeline(text)[0]\n",
    "    label = label_map[result[\"label\"]]\n",
    "    print(f\"Text: {text}\\nSentiment: {label}, Score: {result['score']:.4f}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "38570bc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "model_path = r\"F:\\python\\model\\cardiffnlp_sentiment\"\n",
    "\n",
    "# Load tokenizer and model from local directory\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "\n",
    "# Create sentiment analysis pipeline\n",
    "sentiment_pipeline = pipeline(\"text-classification\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "sentence= \"I love working with Hugging Face!\"\n",
    "\n",
    "result=sentiment_pipeline(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e12b32fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=result[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0978762a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LABEL_2'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dcdbd052",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\saumi\\anaconda3\\envs\\langchain-llama\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\saumi\\.cache\\huggingface\\hub\\models--tabularisai--multilingual-sentiment-analysis. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: I absolutely love the new design of this app!\n",
      "Sentiment: Very Positive\n",
      "\n",
      "Text: The customer service was disappointing.\n",
      "Sentiment: Negative\n",
      "\n",
      "Text: The weather is fine, nothing special.\n",
      "Sentiment: Neutral\n",
      "\n",
      "Text: 这家餐厅的菜味道非常棒！\n",
      "Sentiment: Very Positive\n",
      "\n",
      "Text: 我对他的回答很失望。\n",
      "Sentiment: Negative\n",
      "\n",
      "Text: 天气今天一般。\n",
      "Sentiment: Neutral\n",
      "\n",
      "Text: ¡Me encanta cómo quedó la decoración!\n",
      "Sentiment: Very Positive\n",
      "\n",
      "Text: El servicio fue terrible y muy lento.\n",
      "Sentiment: Very Negative\n",
      "\n",
      "Text: El libro estuvo más o menos.\n",
      "Sentiment: Neutral\n",
      "\n",
      "Text: الخدمة في هذا الفندق رائعة جدًا!\n",
      "Sentiment: Very Positive\n",
      "\n",
      "Text: لم يعجبني الطعام في هذا المطعم.\n",
      "Sentiment: Negative\n",
      "\n",
      "Text: كانت الرحلة عادية。\n",
      "Sentiment: Neutral\n",
      "\n",
      "Text: Мені дуже сподобалася ця вистава!\n",
      "Sentiment: Very Positive\n",
      "\n",
      "Text: Обслуговування було жахливим.\n",
      "Sentiment: Very Negative\n",
      "\n",
      "Text: Книга була посередньою。\n",
      "Sentiment: Negative\n",
      "\n",
      "Text: यह जगह सच में अद्भुत है!\n",
      "Sentiment: Very Positive\n",
      "\n",
      "Text: यह अनुभव बहुत खराब था।\n",
      "Sentiment: Very Negative\n",
      "\n",
      "Text: फिल्म ठीक-ठाक थी।\n",
      "Sentiment: Neutral\n",
      "\n",
      "Text: এখানকার পরিবেশ অসাধারণ!\n",
      "Sentiment: Very Positive\n",
      "\n",
      "Text: সেবার মান একেবারেই খারাপ।\n",
      "Sentiment: Very Negative\n",
      "\n",
      "Text: খাবারটা মোটামুটি ছিল।\n",
      "Sentiment: Neutral\n",
      "\n",
      "Text: Este livro é fantástico! Eu aprendi muitas coisas novas e inspiradoras.\n",
      "Sentiment: Very Positive\n",
      "\n",
      "Text: Não gostei do produto, veio quebrado.\n",
      "Sentiment: Very Negative\n",
      "\n",
      "Text: O filme foi ok, nada de especial.\n",
      "Sentiment: Neutral\n",
      "\n",
      "Text: このレストランの料理は本当に美味しいです！\n",
      "Sentiment: Very Positive\n",
      "\n",
      "Text: このホテルのサービスはがっかりしました。\n",
      "Sentiment: Negative\n",
      "\n",
      "Text: 天気はまあまあです。\n",
      "Sentiment: Neutral\n",
      "\n",
      "Text: Я в восторге от этого нового гаджета!\n",
      "Sentiment: Very Positive\n",
      "\n",
      "Text: Этот сервис оставил у меня только разочарование.\n",
      "Sentiment: Very Negative\n",
      "\n",
      "Text: Встреча была обычной, ничего особенного.\n",
      "Sentiment: Neutral\n",
      "\n",
      "Text: J'adore ce restaurant, c'est excellent !\n",
      "Sentiment: Very Positive\n",
      "\n",
      "Text: L'attente était trop longue et frustrante.\n",
      "Sentiment: Negative\n",
      "\n",
      "Text: Le film était moyen, sans plus.\n",
      "Sentiment: Neutral\n",
      "\n",
      "Text: Bu otelin manzarasına bayıldım!\n",
      "Sentiment: Very Positive\n",
      "\n",
      "Text: Ürün tam bir hayal kırıklığıydı.\n",
      "Sentiment: Very Negative\n",
      "\n",
      "Text: Konser fena değildi, ortalamaydı.\n",
      "Sentiment: Neutral\n",
      "\n",
      "Text: Adoro questo posto, è fantastico!\n",
      "Sentiment: Very Positive\n",
      "\n",
      "Text: Il servizio clienti è stato pessimo.\n",
      "Sentiment: Very Negative\n",
      "\n",
      "Text: La cena era nella media.\n",
      "Sentiment: Neutral\n",
      "\n",
      "Text: Uwielbiam tę restaurację, jedzenie jest świetne!\n",
      "Sentiment: Very Positive\n",
      "\n",
      "Text: Obsługa klienta była rozczarowująca.\n",
      "Sentiment: Negative\n",
      "\n",
      "Text: Pogoda jest w porządku, nic szczególnego.\n",
      "Sentiment: Neutral\n",
      "\n",
      "Text: Ang ganda ng lugar na ito, sobrang aliwalas!\n",
      "Sentiment: Very Positive\n",
      "\n",
      "Text: Hindi maganda ang serbisyo nila dito.\n",
      "Sentiment: Negative\n",
      "\n",
      "Text: Maayos lang ang palabas, walang espesyal.\n",
      "Sentiment: Neutral\n",
      "\n",
      "Text: Ik ben echt blij met mijn nieuwe aankoop!\n",
      "Sentiment: Very Positive\n",
      "\n",
      "Text: De klantenservice was echt slecht.\n",
      "Sentiment: Negative\n",
      "\n",
      "Text: De presentatie was gewoon oké, niet bijzonder.\n",
      "Sentiment: Neutral\n",
      "\n",
      "Text: Saya suka makanan di sini, sangat sedap!\n",
      "Sentiment: Very Positive\n",
      "\n",
      "Text: Pengalaman ini sangat mengecewakan.\n",
      "Sentiment: Very Negative\n",
      "\n",
      "Text: Hari ini cuacanya biasa sahaja.\n",
      "Sentiment: Neutral\n",
      "\n",
      "Text: 이 가게의 케이크는 정말 맛있어요!\n",
      "Sentiment: Very Positive\n",
      "\n",
      "Text: 서비스가 너무 별로였어요.\n",
      "Sentiment: Negative\n",
      "\n",
      "Text: 날씨가 그저 그렇네요.\n",
      "Sentiment: Neutral\n",
      "\n",
      "Text: Ich find dä Service i de Beiz mega guet!\n",
      "Sentiment: Positive\n",
      "\n",
      "Text: Däs Esä het mir nöd gfalle.\n",
      "Sentiment: Very Negative\n",
      "\n",
      "Text: D Wätter hüt isch so naja.\n",
      "Sentiment: Negative\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "model_name = \"tabularisai/multilingual-sentiment-analysis\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "def predict_sentiment(texts):\n",
    "    inputs = tokenizer(texts, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "    sentiment_map = {0: \"Very Negative\", 1: \"Negative\", 2: \"Neutral\", 3: \"Positive\", 4: \"Very Positive\"}\n",
    "    return [sentiment_map[p] for p in torch.argmax(probabilities, dim=-1).tolist()]\n",
    "\n",
    "texts = [\n",
    "    # English\n",
    "    \"I absolutely love the new design of this app!\", \"The customer service was disappointing.\", \"The weather is fine, nothing special.\",\n",
    "    # Chinese\n",
    "    \"这家餐厅的菜味道非常棒！\", \"我对他的回答很失望。\", \"天气今天一般。\",\n",
    "    # Spanish\n",
    "    \"¡Me encanta cómo quedó la decoración!\", \"El servicio fue terrible y muy lento.\", \"El libro estuvo más o menos.\",\n",
    "    # Arabic\n",
    "    \"الخدمة في هذا الفندق رائعة جدًا!\", \"لم يعجبني الطعام في هذا المطعم.\", \"كانت الرحلة عادية。\",\n",
    "    # Ukrainian\n",
    "    \"Мені дуже сподобалася ця вистава!\", \"Обслуговування було жахливим.\", \"Книга була посередньою。\",\n",
    "    # Hindi\n",
    "    \"यह जगह सच में अद्भुत है!\", \"यह अनुभव बहुत खराब था।\", \"फिल्म ठीक-ठाक थी।\",\n",
    "    # Bengali\n",
    "    \"এখানকার পরিবেশ অসাধারণ!\", \"সেবার মান একেবারেই খারাপ।\", \"খাবারটা মোটামুটি ছিল।\",\n",
    "    # Portuguese\n",
    "    \"Este livro é fantástico! Eu aprendi muitas coisas novas e inspiradoras.\", \n",
    "    \"Não gostei do produto, veio quebrado.\", \"O filme foi ok, nada de especial.\",\n",
    "    # Japanese\n",
    "    \"このレストランの料理は本当に美味しいです！\", \"このホテルのサービスはがっかりしました。\", \"天気はまあまあです。\",\n",
    "    # Russian\n",
    "    \"Я в восторге от этого нового гаджета!\", \"Этот сервис оставил у меня только разочарование.\", \"Встреча была обычной, ничего особенного.\",\n",
    "    # French\n",
    "    \"J'adore ce restaurant, c'est excellent !\", \"L'attente était trop longue et frustrante.\", \"Le film était moyen, sans plus.\",\n",
    "    # Turkish\n",
    "    \"Bu otelin manzarasına bayıldım!\", \"Ürün tam bir hayal kırıklığıydı.\", \"Konser fena değildi, ortalamaydı.\",\n",
    "    # Italian\n",
    "    \"Adoro questo posto, è fantastico!\", \"Il servizio clienti è stato pessimo.\", \"La cena era nella media.\",\n",
    "    # Polish\n",
    "    \"Uwielbiam tę restaurację, jedzenie jest świetne!\", \"Obsługa klienta była rozczarowująca.\", \"Pogoda jest w porządku, nic szczególnego.\",\n",
    "    # Tagalog\n",
    "    \"Ang ganda ng lugar na ito, sobrang aliwalas!\", \"Hindi maganda ang serbisyo nila dito.\", \"Maayos lang ang palabas, walang espesyal.\",\n",
    "    # Dutch\n",
    "    \"Ik ben echt blij met mijn nieuwe aankoop!\", \"De klantenservice was echt slecht.\", \"De presentatie was gewoon oké, niet bijzonder.\",\n",
    "    # Malay\n",
    "    \"Saya suka makanan di sini, sangat sedap!\", \"Pengalaman ini sangat mengecewakan.\", \"Hari ini cuacanya biasa sahaja.\",\n",
    "    # Korean\n",
    "    \"이 가게의 케이크는 정말 맛있어요!\", \"서비스가 너무 별로였어요.\", \"날씨가 그저 그렇네요.\",\n",
    "    # Swiss German\n",
    "    \"Ich find dä Service i de Beiz mega guet!\", \"Däs Esä het mir nöd gfalle.\", \"D Wätter hüt isch so naja.\"\n",
    "]\n",
    "\n",
    "for text, sentiment in zip(texts, predict_sentiment(texts)):\n",
    "    print(f\"Text: {text}\\nSentiment: {sentiment}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d1969947",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of India is **New Delhi**. It is a city located within the **National Capital Territory of Delhi** (NCT) and serves as the political and administrative center of the country. New Delhi was officially declared the capital in 1911, replacing Calcutta (now Kolkata), and has since been the seat of the Government of India, housing landmarks such as the **Rashtrapati Bhavan** (President's House), **Parliament House**, and **India Gate**. \n",
      "\n",
      "While Delhi as a region holds historical and cultural significance, New Delhi specifically is the modern capital. The city is renowned for its blend of tradition and modernity, as well as its role as a hub for government, diplomacy, and international events like the **Commonwealth Games**.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from typing import Optional\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.utils.utils import secret_from_env\n",
    "from langchain_openai import ChatOpenAI\n",
    "from pydantic import Field, SecretStr\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "class ChatOpenRouter(ChatOpenAI):\n",
    "    openai_api_key: Optional[SecretStr] = Field(\n",
    "        alias=\"api_key\",\n",
    "        default_factory=secret_from_env(\"OPENROUTER_API_KEY\", default=None),\n",
    "    )\n",
    "    @property\n",
    "    def lc_secrets(self) -> dict[str, str]:\n",
    "        return {\"openai_api_key\": \"OPENROUTER_API_KEY\"}\n",
    "\n",
    "    def __init__(self,\n",
    "                 openai_api_key: Optional[str] = None,\n",
    "                 **kwargs):\n",
    "        openai_api_key = (\n",
    "            openai_api_key or os.environ.get(\"OPENROUTER_API_KEY\")\n",
    "        )\n",
    "        super().__init__(\n",
    "            base_url=\"https://openrouter.ai/api/v1\",\n",
    "            openai_api_key=openai_api_key,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "model = ChatOpenRouter(\n",
    "    model_name=\"qwen/qwen3-14b:free\"\n",
    ")\n",
    "\n",
    "result = model.invoke(\"What is the capital of India\")\n",
    "\n",
    "print(result.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebae7818",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f4d1a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861e61a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898b07c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\saumi\\anaconda3\\envs\\langchain-llama\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Fetching 15 files:   0%|          | 0/15 [00:00<?, ?it/s]c:\\Users\\saumi\\anaconda3\\envs\\langchain-llama\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\saumi\\.cache\\huggingface\\hub\\models--instruction-tuning-sd--cartoonizer. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Fetching 15 files: 100%|██████████| 15/15 [08:53<00:00, 35.57s/it]\n",
      "Keyword arguments {'use_auth_token': True} are not expected by StableDiffusionInstructPix2PixPipeline and will be ignored.\n",
      "Loading pipeline components...:  43%|████▎     | 3/7 [00:03<00:04,  1.19s/it]An error occurred while trying to fetch C:\\Users\\saumi\\.cache\\huggingface\\hub\\models--instruction-tuning-sd--cartoonizer\\snapshots\\94ae1467bb3a3b231cb444ab0acd4295836014f1\\vae: Error no file named diffusion_pytorch_model.safetensors found in directory C:\\Users\\saumi\\.cache\\huggingface\\hub\\models--instruction-tuning-sd--cartoonizer\\snapshots\\94ae1467bb3a3b231cb444ab0acd4295836014f1\\vae.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n",
      "Loading pipeline components...:  86%|████████▌ | 6/7 [00:10<00:02,  2.31s/it]An error occurred while trying to fetch C:\\Users\\saumi\\.cache\\huggingface\\hub\\models--instruction-tuning-sd--cartoonizer\\snapshots\\94ae1467bb3a3b231cb444ab0acd4295836014f1\\unet: Error no file named diffusion_pytorch_model.safetensors found in directory C:\\Users\\saumi\\.cache\\huggingface\\hub\\models--instruction-tuning-sd--cartoonizer\\snapshots\\94ae1467bb3a3b231cb444ab0acd4295836014f1\\unet.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n",
      "Loading pipeline components...: 100%|██████████| 7/7 [00:18<00:00,  2.62s/it]\n",
      "Pipelines loaded with `dtype=torch.float16` cannot run with `cpu` device. It is not recommended to move them to `cpu` as running them will fail. Please make sure to use an accelerator to run the pipeline in inference, due to the lack of support for`float16` operations on this device in PyTorch. Please, remove the `torch_dtype=torch.float16` argument, or use another device for inference.\n",
      "Pipelines loaded with `dtype=torch.float16` cannot run with `cpu` device. It is not recommended to move them to `cpu` as running them will fail. Please make sure to use an accelerator to run the pipeline in inference, due to the lack of support for`float16` operations on this device in PyTorch. Please, remove the `torch_dtype=torch.float16` argument, or use another device for inference.\n",
      "Pipelines loaded with `dtype=torch.float16` cannot run with `cpu` device. It is not recommended to move them to `cpu` as running them will fail. Please make sure to use an accelerator to run the pipeline in inference, due to the lack of support for`float16` operations on this device in PyTorch. Please, remove the `torch_dtype=torch.float16` argument, or use another device for inference.\n",
      "Pipelines loaded with `dtype=torch.float16` cannot run with `cpu` device. It is not recommended to move them to `cpu` as running them will fail. Please make sure to use an accelerator to run the pipeline in inference, due to the lack of support for`float16` operations on this device in PyTorch. Please, remove the `torch_dtype=torch.float16` argument, or use another device for inference.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from diffusers import StableDiffusionInstructPix2PixPipeline\n",
    "from diffusers.utils import load_image\n",
    "\n",
    "model_id = \"instruction-tuning-sd/cartoonizer\"\n",
    "pipeline = StableDiffusionInstructPix2PixPipeline.from_pretrained(\n",
    "    model_id, torch_dtype=torch.float16, use_auth_token=True\n",
    ").to(\"cpu\")\n",
    "\n",
    "image_path = r\"C:\\Users\\saumi\\Downloads\\donald.jpeg\"\n",
    "image = load_image(image_path)\n",
    "\n",
    "image = pipeline(\"Cartoonize the following image\", image=image).images[0]\n",
    "image.save(\"image.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8be25da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-llama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
